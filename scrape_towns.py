import hrequests
from bs4 import BeautifulSoup
import sqlite3
import re
import time

DATABASE_NAME = 'hungarian_towns.db'
BASE_WIKI_URL = 'https://hu.wikipedia.org'
TOWN_LIST_URL = 'https://hu.wikipedia.org/wiki/Magyarorsz%C3%A1g_telep%C3%BCl%C3%A9sei:_A,_%C3%81'

def setup_database():
    """Sets up the SQLite database and table."""
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS towns (
            name TEXT PRIMARY KEY,
            type TEXT,
            county TEXT,
            kisterseg TEXT,
            jaras TEXT,
            population INTEGER,
            zip_code TEXT,
            mayor TEXT,
            latitude REAL,
            longitude REAL
        )
    ''')
    conn.commit()
    conn.close()

# --- NEW HELPER FUNCTION ---
def get_completed_towns():
    """Queries the database and returns a set of names of towns that have already been scraped."""
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    try:
        # We can just check for the name, since the name is the PRIMARY KEY.
        # Any entry means it has been processed.
        cursor.execute("SELECT name FROM towns")
        completed_towns = {row[0] for row in cursor.fetchall()}
        return completed_towns
    except sqlite3.Error as e:
        print(f"Database error while fetching completed towns: {e}")
        return set()
    finally:
        conn.close()

def get_soup(url):
    """Fetches a URL and returns a BeautifulSoup object."""
    try:
        # Adding a timeout is good practice for network requests
        response = hrequests.get(url, timeout=15)
        if response.status_code != 200:
            print(f"Failed to fetch {url}. Status code: {response.status_code}")
            return None
        return BeautifulSoup(response.text, 'html.parser')
    except hrequests.exceptions.ClientException as e:
        print(f"Error fetching {url}: {e}")
        return None

def scrape_town_list_page(soup):
    """
    Scrapes a single town list page for town details and links.
    This version is robust and handles tables with missing or reordered columns,
    and also tables where the <thead> tag is generated by JavaScript.
    """
    towns_data = []
    table = soup.find('table', class_='wikitable sortable')

    if not table:
        print("Could not find the main town table on the page.")
        return towns_data

    all_rows = table.find_all('tr')
    if not all_rows:
        print("Table has no rows. Skipping.")
        return towns_data

    header_row = all_rows[0]
    data_rows = all_rows[1:]

    headers = [th.get_text(strip=True) for th in header_row.find_all('th')]
    
    header_map = {}
    key_mapping = {
        'name': ['Név'], 'type': ['Típus'], 'county': ['Vármegye'],
        'kisterseg': ['Kistérség'], 'jaras': ['Járás'], 'population': ['Nép.'],
        'zip_code': ['Irsz.']
    }

    for key, possible_headers in key_mapping.items():
        for i, header_text in enumerate(headers):
            for p_header in possible_headers:
                if p_header in header_text:
                    header_map[key] = i
                    break
    
    if 'name' not in header_map:
        print("Could not find the 'Név' (Name) column header. Skipping table.")
        return towns_data
    
    for row in data_rows:
        cols = row.find_all('td')
        if not cols: continue

        def get_col_text(key):
            index = header_map.get(key)
            return cols[index].get_text(strip=True) if index is not None and index < len(cols) else None

        town_name_col_index = header_map.get('name')
        if town_name_col_index is None or town_name_col_index >= len(cols): continue
            
        town_name_tag = cols[town_name_col_index].find('a')
        if town_name_tag:
            town_name = town_name_tag.get_text(strip=True)
            town_link = town_name_tag['href']
            population_str = get_col_text('population')
            zip_code_raw = get_col_text('zip_code')

            towns_data.append({
                'name': town_name,
                'link': BASE_WIKI_URL + town_link,
                'type': get_col_text('type'),
                'county': get_col_text('county'),
                'kisterseg': get_col_text('kisterseg'),
                'jaras': get_col_text('jaras'),
                'population': int(population_str.replace('.', '')) if population_str and population_str.replace('.', '').isdigit() else None,
                'zip_code': zip_code_raw.split('–')[0].split('-')[0].strip() if zip_code_raw else None,
                'mayor': None, 'latitude': None, 'longitude': None
            })
    return towns_data

def scrape_individual_town_page(town_url):
    """
    Scrapes an individual town page for mayor's name and GPS coordinates.
    Returns a tuple (mayor, latitude, longitude).
    """
    soup = get_soup(town_url)
    if not soup:
        return None, None, None

    mayor, latitude, longitude = None, None, None
    infobox = soup.find('table', class_='infobox ujinfobox')
    if infobox:
        mayor_row = infobox.find('td', class_='cimke', string='Polgármester')
        if mayor_row and mayor_row.find_next_sibling('td'):
            mayor = mayor_row.find_next_sibling('td').get_text(strip=True).split('(')[0].strip()

        geo_span = infobox.find('span', class_='geo')
        if geo_span:
            lat_span = geo_span.find('span', class_='latitude')
            lon_span = geo_span.find('span', class_='longitude')
            if lat_span and lon_span:
                try:
                    dec_coords = geo_span.find('span', class_='geo-dec')
                    if dec_coords:
                        match = re.search(r'(-?\d+\.?\d*)°[NS]\s*(-?\d+\.?\d*)°[EW]', dec_coords.get_text())
                        if match:
                            latitude, longitude = float(match.group(1)), float(match.group(2))
                    
                    if latitude is None: # Fallback to DMS
                        latitude = convert_dms_to_decimal(lat_span.get_text(strip=True))
                        longitude = convert_dms_to_decimal(lon_span.get_text(strip=True))
                except (ValueError, IndexError):
                    print(f"Could not parse GPS for {town_url}")
    return mayor, latitude, longitude

def convert_dms_to_decimal(dms_str):
    """Converts a DMS string (e.g., 'é. sz. 47° 01′ 50″') to decimal degrees."""
    clean_dms_str = re.sub(r'[é\. sz\.k\.h°′″]', '', dms_str)
    parts = [float(p) for p in clean_dms_str.split() if p]
    degrees = parts[0] if len(parts) > 0 else 0
    minutes = parts[1] if len(parts) > 1 else 0
    seconds = parts[2] if len(parts) > 2 else 0
    decimal = degrees + minutes / 60 + seconds / 3600
    if 'd.' in dms_str or 'ny.' in dms_str: decimal = -decimal
    return decimal

def insert_town_data(town_data):
    """Inserts a single town's data into the database."""
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    try:
        # Instead of using .values(), we explicitly list the values in the correct order.
        # This ensures we only pass the 10 values that match the 10 columns.
        cursor.execute('''
            INSERT OR REPLACE INTO towns (name, type, county, kisterseg, jaras, population, zip_code, mayor, latitude, longitude)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            town_data['name'],
            town_data['type'],
            town_data['county'],
            town_data['kisterseg'],
            town_data['jaras'],
            town_data['population'],
            town_data['zip_code'],
            town_data['mayor'],
            town_data['latitude'],
            town_data['longitude']
        ))
        conn.commit()
    except sqlite3.Error as e:
        print(f"Error inserting data for {town_data['name']}: {e}")
    finally:
        conn.close()

# --- REFACTORED MAIN FUNCTION ---
def main():
    setup_database()
    
    # Step 1: Get the set of towns that are already completed.
    completed_towns = get_completed_towns()
    print(f"Found {len(completed_towns)} towns already in the database. Resuming...")

    # Step 2: Gather the full list of all towns from Wikipedia (this part is fast).
    main_soup = get_soup(TOWN_LIST_URL)
    if not main_soup:
        return

    letter_links = []
    toc_table = main_soup.find('table', id='toc')
    if toc_table:
        for a_tag in toc_table.find_all('a', href=True):
            if a_tag['href'].startswith('/wiki/Magyarorsz%C3%A1g_telep%C3%BCl%C3%A9sei:'):
                letter_links.append(BASE_WIKI_URL + a_tag['href'])
    
    if TOWN_LIST_URL not in letter_links:
        letter_links.insert(0, TOWN_LIST_URL)

    all_towns = []
    for link in sorted(list(set(letter_links))):
        time.sleep(0.5)
        print(f"Gathering town list from: {link}")
        list_soup = get_soup(link)
        if list_soup:
            towns_on_page = scrape_town_list_page(list_soup)
            all_towns.extend(towns_on_page)

    print(f"Found a total of {len(all_towns)} towns. Now processing individual pages...")

    # Step 3: Iterate through all towns and skip the ones already completed.
    for i, town in enumerate(all_towns):
        
        # This is the core of the resume logic
        if town['name'] in completed_towns:
            # Optional: uncomment the line below for verbose skipping messages
            # print(f"({i+1}/{len(all_towns)}) Skipping {town['name']}, already processed.")
            continue
        
        # If not skipped, process the town as before
        print(f"({i+1}/{len(all_towns)}) Processing {town['name']} from {town['link']}")
        time.sleep(0.4) # A small delay to be polite to Wikipedia's servers
        
        mayor, latitude, longitude = scrape_individual_town_page(town['link'])
        
        # We need to update the original town dictionary before inserting
        town['mayor'] = mayor
        town['latitude'] = latitude
        town['longitude'] = longitude
        
        insert_town_data(town)
    
    print("Scraping complete. Data stored in hungarian_towns.db")

if __name__ == '__main__':
    main()